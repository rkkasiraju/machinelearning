# Difference Between Linear and Logistic Regression

In this video, we explore the differences between linear regression and logistic regression. We start by revisiting the telecommunication dataset and examine how each method can be used to solve different problems.

## Linear Regression for Continuous Prediction

First, let's consider linear regression for continuous prediction. In this scenario, we aim to predict a continuous variable like income. We choose an independent variable, such as customer age, and predict the dependent variable, income. Linear regression fits a line or polynomial through the data, and the equation of the line is represented as Y = a + b * x1.

## Linear Regression for Binary Classification

Now, let's see if we can use linear regression for binary classification, like predicting churn (yes or no). We represent class 0 (churn is no) in red and class 1 (churn is yes) in blue on a scatterplot. The linear regression model will still fit a line, which is problematic for classification. We introduce a threshold (0.5) to classify records into classes. However, this approach lacks the ability to provide probability estimates.

## Introduction to the Sigmoid Function

Instead of using linear regression, we introduce the sigmoid function, which is a critical component of logistic regression. The sigmoid function returns values between 0 and 1, making it suitable for estimating probabilities. The sigmoid function is expressed as 1 / (1 + e^(-Theta * x)), where e is the base of the natural logarithm.

## Role of the Sigmoid Function

The sigmoid function maps large positive or negative values of Theta transpose x to values close to 1 or 0, respectively. This is ideal for modeling the probability of an event, such as a customer churning. When the sigmoid output is close to 1, it indicates a high probability of the event occurring, while values close to 0 represent a low probability.

## Using Sigmoid in Logistic Regression

Logistic regression uses the sigmoid function to model the probability of an input belonging to a class. We aim to find the optimal parameters (Theta) that maximize the likelihood of the actual class labels given the input features. The training process involves adjusting the parameters to minimize the cost function, which measures the difference between actual and predicted labels.

## Iterative Training Process

The training process consists of iterations, where we initialize Theta with random values, calculate the model output (probability), compute the error for each customer, and update Theta to minimize the total cost. This process continues until the cost is sufficiently low.

## Gradient Descent and Stopping Criteria

Gradient descent is commonly used to adjust Theta values during training. To stop the iterations, we typically calculate the model's accuracy and stop when it meets our criteria.

In summary, logistic regression and the sigmoid function allow us to estimate probabilities and make binary classifications. Linear regression, although suitable for continuous prediction, is not appropriate for classification problems.
